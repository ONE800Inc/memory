{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the test for memory using Langchain and other libraries \n",
    "\n",
    "For now, here will be focused on only the short term and long term memories using Langchain. Knowdledge based memories will be explored in the later version \n",
    "\n",
    "Also note that the python environment it is currently running is Python3.11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Langchain "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of different memories: \n",
    "- ConversationBufferMemory\n",
    "- ConversationSummaryMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationSummaryBufferMemory\n",
    "- Conversation Knowledge Graph Memory \n",
    "- Entity Memory \n",
    "- Custom Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 -q install openai langchain huggingface_hub transformers "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Conversation with ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in deps \n",
    "\n",
    "import os \n",
    "from langchain_testings.apikey import API_KEY\n",
    "\n",
    "import streamlit as st \n",
    "from langchain.llms import OpenAI \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "st.title('Youtube GPT Creator')\n",
    "prompt = st.text_input('plug in your prompt here ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory \n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name = 'text-davinci-003',\n",
    "             temperature = 0.8,\n",
    "             max_tokens = 256)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    verbose = True,\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Jerry Cheng\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Jerry! It's nice to meet you. My name is AI. What can I help you with?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = \"Hi, my name is Jerry Cheng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Jerry Cheng\n",
      "AI:  Hi Jerry! It's nice to meet you. My name is AI. What can I help you with?\n",
      "Human: I am a Mechanical Engineering student\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Wonderful! Mechanical Engineering is a fascinating field. What sort of projects are you currently working on?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = \"I am a Mechanical Engineering student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Jerry Cheng\n",
      "AI:  Hi Jerry! It's nice to meet you. My name is AI. What can I help you with?\n",
      "Human: I am a Mechanical Engineering student\n",
      "AI:  Wonderful! Mechanical Engineering is a fascinating field. What sort of projects are you currently working on?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Conversation with ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name = 'text-davinci-003',\n",
    "             temperature = 0.8,\n",
    "             max_tokens = 256)\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm = OpenAI())\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    verbose = True,\n",
    "    memory = summary_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input = \"Hi my name is Jerry Cheng\")\n",
    "conversation.predict(input = \"I am a mechanical engineering student\")\n",
    "conversation.predict(input = 'i am designing a autonomous commercialized robotic arm that help with household chroses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human introduces himself as Jerry Cheng and the AI greets him by name. AI offers assistance, asking how it may help Jerry, and upon discovering Jerry is a mechanical engineering student, it inquires about the topics he is studying. Jerry explains he is designing a autonomous commercialized robotic arm that helps with household chores, to which the AI responds with enthusiasm and inquires further about the project.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic conversation with ConversationBufferWindowMemory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name = 'text-davinci-003',\n",
    "             temperature = 0.8,\n",
    "             max_tokens = 256)\n",
    "\n",
    "window_memory = ConversationBufferWindowMemory(k = 2)\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm = llm,\n",
    "    verbose = True,\n",
    "    memory = window_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi my name is Jerry Cheng\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi my name is Jerry Cheng\n",
      "AI:  Hi Jerry, my name is AI. It's nice to meet you. What brings you here today?\n",
      "Human: I am a mechanical engineering student\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi my name is Jerry Cheng\n",
      "AI:  Hi Jerry, my name is AI. It's nice to meet you. What brings you here today?\n",
      "Human: I am a mechanical engineering student\n",
      "AI:  Nice to meet you, Jerry! It sounds like you're studying a fascinating field. What kind of projects have you been working on lately?\n",
      "Human: i am designing a autonomous commercialized robotic arm that help with household chroses\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like a really cool project! What challenges have you faced in designing this robotic arm?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input = \"Hi my name is Jerry Cheng\")\n",
    "conversation_with_summary.predict(input = \"I am a mechanical engineering student\")\n",
    "conversation_with_summary.predict(input = 'i am designing a autonomous commercialized robotic arm that help with household chroses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(conversation_with_summary.memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Conversation with ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name='text-davinci-003', \n",
    "             temperature=0, \n",
    "             max_tokens = 512)\n",
    "\n",
    "# Setting k=2, will only keep the last 2 interactions in memory\n",
    "# max_token_limit=40 - token limits needs transformers installed\n",
    "memory = ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=40) \n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory, \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Sam\n",
      "AI:  Hi Sam! My name is AI. It's nice to meet you. What brings you here today?\n",
      "Human: Hi my name is Jerry Cheng\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human, Sam, introduces themselves to the AI. The AI greets Sam and asks what brings them here today.\n",
      "Human: Hi my name is Jerry Cheng\n",
      "AI:  Hi Jerry! It's nice to meet you. What can I do for you today?\n",
      "Human: I am a mechanical engineering student\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human, Jerry, introduces themselves to the AI. The AI greets Jerry and asks what can it do for them today.\n",
      "Human: I am a mechanical engineering student\n",
      "AI:  That's great to hear, Jerry! What kind of projects are you working on in your mechanical engineering studies?\n",
      "Human: i am designing a autonomous commercialized robotic arm that help with household chroses\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Wow, that sounds like an amazing project! What challenges have you faced so far in designing the robotic arm?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input = \"Hi my name is Jerry Cheng\")\n",
    "conversation_with_summary.predict(input = \"I am a mechanical engineering student\")\n",
    "conversation_with_summary.predict(input = 'i am designing a autonomous commercialized robotic arm that help with household chroses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human, Jerry, introduces themselves to the AI. The AI greets Jerry and asks what it can do for them today. Jerry is a mechanical engineering student and is designing a autonomous robotic arm for commercial use for household chores.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_with_summary.memory.moving_summary_buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Conversation with Conversation Knowledge Graph Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationKGMemory\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name='text-davinci-003', \n",
    "             temperature=0, \n",
    "             max_tokens = 256)\n",
    "\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=template\n",
    ")\n",
    "\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    prompt=prompt,\n",
    "    memory=ConversationKGMemory(llm=llm)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: Hi my name is Jerry Cheng\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: I am a mechanical engineering student\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "On Jerry Cheng: Jerry Cheng is a mechanical engineering student.\n",
      "\n",
      "Conversation:\n",
      "Human: i am designing a autonomous commercialized robotic arm that help with household chroses\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like an interesting project! Can you tell me more about it? What kind of tasks do you envision the robotic arm performing?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input = \"Hi my name is Jerry Cheng\")\n",
    "conversation_with_kg.predict(input = \"I am a mechanical engineering student\")\n",
    "conversation_with_kg.predict(input = 'i am designing a autonomous commercialized robotic arm that help with household chroses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp311-cp311-macosx_11_0_arm64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.0.7-cp311-cp311-macosx_11_0_arm64.whl (229 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.4-cp311-cp311-macosx_11_0_arm64.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jerrycheng/Library/Python/3.11/lib/python/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jerrycheng/Library/Python/3.11/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jerrycheng/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.39.4 kiwisolver-1.4.4 matplotlib-3.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NetworkxEntityGraph' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnetworkx\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m nx\u001b[39m.\u001b[39;49mdraw(conversation_with_kg\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49mkg, with_labels\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      6\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(conversation_with_kg\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mkg)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/networkx/drawing/nx_pylab.py:121\u001b[0m, in \u001b[0;36mdraw\u001b[0;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mwith_labels\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwds:\n\u001b[1;32m    119\u001b[0m     kwds[\u001b[39m\"\u001b[39m\u001b[39mwith_labels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds\n\u001b[0;32m--> 121\u001b[0m draw_networkx(G, pos\u001b[39m=\u001b[39;49mpos, ax\u001b[39m=\u001b[39;49max, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    122\u001b[0m ax\u001b[39m.\u001b[39mset_axis_off()\n\u001b[1;32m    123\u001b[0m plt\u001b[39m.\u001b[39mdraw_if_interactive()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/networkx/drawing/nx_pylab.py:301\u001b[0m, in \u001b[0;36mdraw_networkx\u001b[0;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[1;32m    298\u001b[0m label_kwds \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwds\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m valid_label_kwds}\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m pos \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     pos \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39;49mdrawing\u001b[39m.\u001b[39;49mspring_layout(G)  \u001b[39m# default to spring layout\u001b[39;00m\n\u001b[1;32m    303\u001b[0m draw_networkx_nodes(G, pos, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnode_kwds)\n\u001b[1;32m    304\u001b[0m draw_networkx_edges(G, pos, arrows\u001b[39m=\u001b[39marrows, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39medge_kwds)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/networkx/utils/decorators.py:766\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[0;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(\u001b[39m*\u001b[39margs, __wrapper\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 766\u001b[0m     \u001b[39mreturn\u001b[39;00m argmap\u001b[39m.\u001b[39;49m_lazy_compile(__wrapper)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 4:4\u001b[0m, in \u001b[0;36margmap_spring_layout_1\u001b[0;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/networkx/drawing/layout.py:443\u001b[0m, in \u001b[0;36mspring_layout\u001b[0;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Position nodes using Fruchterman-Reingold force-directed algorithm.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[39mThe algorithm simulates a force-directed representation of the network\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m>>> pos = nx.fruchterman_reingold_layout(G)\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m G, center \u001b[39m=\u001b[39m _process_params(G, center, dim)\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m fixed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m pos \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/networkx/drawing/layout.py:45\u001b[0m, in \u001b[0;36m_process_params\u001b[0;34m(G, center, dim)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(G, nx\u001b[39m.\u001b[39mGraph):\n\u001b[1;32m     44\u001b[0m     empty_graph \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mGraph()\n\u001b[0;32m---> 45\u001b[0m     empty_graph\u001b[39m.\u001b[39;49madd_nodes_from(G)\n\u001b[1;32m     46\u001b[0m     G \u001b[39m=\u001b[39m empty_graph\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m center \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/networkx/classes/graph.py:624\u001b[0m, in \u001b[0;36mGraph.add_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_nodes_from\u001b[39m(\u001b[39mself\u001b[39m, nodes_for_adding, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattr):\n\u001b[1;32m    564\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Add multiple nodes.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \n\u001b[1;32m    566\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39m    >>> G.add_nodes_from(list(n + 1 for n in G.nodes))\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nodes_for_adding:\n\u001b[1;32m    625\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m             newnode \u001b[39m=\u001b[39m n \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_node\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NetworkxEntityGraph' object is not iterable"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr4AAAIRCAYAAACszb5OAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf+0lEQVR4nO3df2zV9b348Ret9lQzW/FyKT9uHVd3ndtUcCC91Rnj0rsmGnb542ZcXYBL/HHduMbR3DtBlM65Ua5XDcnEEZle98e8sBk1yyB4Xe/I4uwNGdDEXUHj0MFd1gp315aLG5X28/1jWfft+CGntAV8PR7J+YO37/f5vI9viU8/nn4YVxRFEQAA8AFXcao3AAAAY0H4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJBC2eH74x//OObMmRNTpkyJcePGxfPPP/++a7Zs2RKf/OQno1QqxUc+8pF46qmnhrFVAAAYvrLD9+DBgzF9+vRYs2bNCc1/880348Ybb4zrr78+Ojs740tf+lLceuut8cILL5S9WQAAGK5xRVEUw148blw899xzMXfu3GPOufvuu2Pjxo3xs5/9bHDsb//2b+Odd96JzZs3D/fSAABQlrNG+wIdHR3R1NQ0ZKy5uTm+9KUvHXPNoUOH4tChQ4O/HhgYiF//+tfxJ3/yJzFu3LjR2ioAAKeJoijiwIEDMWXKlKioGJkfSxv18O3q6oq6urohY3V1ddHb2xu/+c1v4pxzzjliTVtbW9x///2jvTUAAE5ze/fujT/7sz8bkfca9fAdjmXLlkVLS8vgr3t6euLCCy+MvXv3Rk1NzSncGQAAY6G3tzfq6+vjvPPOG7H3HPXwnTRpUnR3dw8Z6+7ujpqamqPe7Y2IKJVKUSqVjhivqakRvgAAiYzk11xH/Tm+jY2N0d7ePmTsxRdfjMbGxtG+NAAADCo7fP/v//4vOjs7o7OzMyJ+97iyzs7O2LNnT0T87msKCxYsGJx/xx13xO7du+PLX/5y7Nq1Kx577LH47ne/G0uWLBmZTwAAACeg7PD96U9/GldeeWVceeWVERHR0tISV155ZaxYsSIiIn71q18NRnBExJ//+Z/Hxo0b48UXX4zp06fHww8/HN/61reiubl5hD4CAAC8v5N6ju9Y6e3tjdra2ujp6fEdXwCABEaj/0b9O74AAHA6EL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIIVhhe+aNWti2rRpUV1dHQ0NDbF169bjzl+9enV89KMfjXPOOSfq6+tjyZIl8dvf/nZYGwYAgOEoO3w3bNgQLS0t0draGtu3b4/p06dHc3NzvP3220ed//TTT8fSpUujtbU1du7cGU888URs2LAh7rnnnpPePAAAnKiyw/eRRx6J2267LRYtWhQf//jHY+3atXHuuefGk08+edT5L7/8clxzzTVx8803x7Rp0+Izn/lM3HTTTe97lxgAAEZSWeHb19cX27Zti6ampj+8QUVFNDU1RUdHx1HXXH311bFt27bB0N29e3ds2rQpbrjhhmNe59ChQ9Hb2zvkBQAAJ+Oscibv378/+vv7o66ubsh4XV1d7Nq166hrbr755ti/f3986lOfiqIo4vDhw3HHHXcc96sObW1tcf/995ezNQAAOK5Rf6rDli1bYuXKlfHYY4/F9u3b49lnn42NGzfGAw88cMw1y5Yti56ensHX3r17R3ubAAB8wJV1x3fChAlRWVkZ3d3dQ8a7u7tj0qRJR11z3333xfz58+PWW2+NiIjLL788Dh48GLfffnssX748KiqObO9SqRSlUqmcrQEAwHGVdce3qqoqZs6cGe3t7YNjAwMD0d7eHo2NjUdd8+677x4Rt5WVlRERURRFufsFAIBhKeuOb0RES0tLLFy4MGbNmhWzZ8+O1atXx8GDB2PRokUREbFgwYKYOnVqtLW1RUTEnDlz4pFHHokrr7wyGhoa4o033oj77rsv5syZMxjAAAAw2soO33nz5sW+fftixYoV0dXVFTNmzIjNmzcP/sDbnj17htzhvffee2PcuHFx7733xi9/+cv40z/905gzZ058/etfH7lPAQAA72NccQZ836C3tzdqa2ujp6cnampqTvV2AAAYZaPRf6P+VAcAADgdCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkMKwwnfNmjUxbdq0qK6ujoaGhti6detx57/zzjuxePHimDx5cpRKpbjkkkti06ZNw9owAAAMx1nlLtiwYUO0tLTE2rVro6GhIVavXh3Nzc3x2muvxcSJE4+Y39fXF3/1V38VEydOjGeeeSamTp0av/jFL+L8888fif0DAMAJGVcURVHOgoaGhrjqqqvi0UcfjYiIgYGBqK+vjzvvvDOWLl16xPy1a9fGv/zLv8SuXbvi7LPPHtYme3t7o7a2Nnp6eqKmpmZY7wEAwJljNPqvrK869PX1xbZt26KpqekPb1BREU1NTdHR0XHUNd///vejsbExFi9eHHV1dXHZZZfFypUro7+//5jXOXToUPT29g55AQDAySgrfPfv3x/9/f1RV1c3ZLyuri66urqOumb37t3xzDPPRH9/f2zatCnuu+++ePjhh+NrX/vaMa/T1tYWtbW1g6/6+vpytgkAAEcY9ac6DAwMxMSJE+Pxxx+PmTNnxrx582L58uWxdu3aY65ZtmxZ9PT0DL727t072tsEAOADrqwfbpswYUJUVlZGd3f3kPHu7u6YNGnSUddMnjw5zj777KisrBwc+9jHPhZdXV3R19cXVVVVR6wplUpRKpXK2RoAABxXWXd8q6qqYubMmdHe3j44NjAwEO3t7dHY2HjUNddcc0288cYbMTAwMDj2+uuvx+TJk48avQAAMBrK/qpDS0tLrFu3Lr797W/Hzp074wtf+EIcPHgwFi1aFBERCxYsiGXLlg3O/8IXvhC//vWv46677orXX389Nm7cGCtXrozFixeP3KcAAID3UfZzfOfNmxf79u2LFStWRFdXV8yYMSM2b948+ANve/bsiYqKP/R0fX19vPDCC7FkyZK44oorYurUqXHXXXfF3XffPXKfAgAA3kfZz/E9FTzHFwAgl1P+HF8AADhTCV8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkMKwwnfNmjUxbdq0qK6ujoaGhti6desJrVu/fn2MGzcu5s6dO5zLAgDAsJUdvhs2bIiWlpZobW2N7du3x/Tp06O5uTnefvvt465766234h//8R/j2muvHfZmAQBguMoO30ceeSRuu+22WLRoUXz84x+PtWvXxrnnnhtPPvnkMdf09/fH5z//+bj//vvjoosuOqkNAwDAcJQVvn19fbFt27Zoamr6wxtUVERTU1N0dHQcc91Xv/rVmDhxYtxyyy0ndJ1Dhw5Fb2/vkBcAAJyMssJ3//790d/fH3V1dUPG6+rqoqur66hrXnrppXjiiSdi3bp1J3ydtra2qK2tHXzV19eXs00AADjCqD7V4cCBAzF//vxYt25dTJgw4YTXLVu2LHp6egZfe/fuHcVdAgCQwVnlTJ4wYUJUVlZGd3f3kPHu7u6YNGnSEfN//vOfx1tvvRVz5swZHBsYGPjdhc86K1577bW4+OKLj1hXKpWiVCqVszUAADiusu74VlVVxcyZM6O9vX1wbGBgINrb26OxsfGI+Zdeemm88sor0dnZOfj67Gc/G9dff310dnb6CgMAAGOmrDu+EREtLS2xcOHCmDVrVsyePTtWr14dBw8ejEWLFkVExIIFC2Lq1KnR1tYW1dXVcdlllw1Zf/7550dEHDEOAACjqezwnTdvXuzbty9WrFgRXV1dMWPGjNi8efPgD7zt2bMnKir8gXAAAJxexhVFUZzqTbyf3t7eqK2tjZ6enqipqTnV2wEAYJSNRv+5NQsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkMKzwXbNmTUybNi2qq6ujoaEhtm7desy569ati2uvvTbGjx8f48ePj6ampuPOBwCA0VB2+G7YsCFaWlqitbU1tm/fHtOnT4/m5uZ4++23jzp/y5YtcdNNN8WPfvSj6OjoiPr6+vjMZz4Tv/zlL0968wAAcKLGFUVRlLOgoaEhrrrqqnj00UcjImJgYCDq6+vjzjvvjKVLl77v+v7+/hg/fnw8+uijsWDBghO6Zm9vb9TW1kZPT0/U1NSUs10AAM5Ao9F/Zd3x7evri23btkVTU9Mf3qCiIpqamqKjo+OE3uPdd9+N9957Ly644IJjzjl06FD09vYOeQEAwMkoK3z3798f/f39UVdXN2S8rq4uurq6Tug97r777pgyZcqQeP5jbW1tUVtbO/iqr68vZ5sAAHCEMX2qw6pVq2L9+vXx3HPPRXV19THnLVu2LHp6egZfe/fuHcNdAgDwQXRWOZMnTJgQlZWV0d3dPWS8u7s7Jk2adNy1Dz30UKxatSp++MMfxhVXXHHcuaVSKUqlUjlbAwCA4yrrjm9VVVXMnDkz2tvbB8cGBgaivb09Ghsbj7nuwQcfjAceeCA2b94cs2bNGv5uAQBgmMq64xsR0dLSEgsXLoxZs2bF7NmzY/Xq1XHw4MFYtGhRREQsWLAgpk6dGm1tbRER8c///M+xYsWKePrpp2PatGmD3wX+0Ic+FB/60IdG8KMAAMCxlR2+8+bNi3379sWKFSuiq6srZsyYEZs3bx78gbc9e/ZERcUfbiR/85vfjL6+vvibv/mbIe/T2toaX/nKV05u9wAAcILKfo7vqeA5vgAAuZzy5/gCAMCZSvgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSELwAAKQhfAABSEL4AAKQgfAEASEH4AgCQgvAFACAF4QsAQArCFwCAFIQvAAApCF8AAFIQvgAApCB8AQBIQfgCAJCC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBSGFb5r1qyJadOmRXV1dTQ0NMTWrVuPO/973/teXHrppVFdXR2XX355bNq0aVibBQCA4So7fDds2BAtLS3R2toa27dvj+nTp0dzc3O8/fbbR53/8ssvx0033RS33HJL7NixI+bOnRtz586Nn/3sZye9eQAAOFHjiqIoylnQ0NAQV111VTz66KMRETEwMBD19fVx5513xtKlS4+YP2/evDh48GD84Ac/GBz7y7/8y5gxY0asXbv2hK7Z29sbtbW10dPTEzU1NeVsFwCAM9Bo9N9Z5Uzu6+uLbdu2xbJlywbHKioqoqmpKTo6Oo66pqOjI1paWoaMNTc3x/PPP3/M6xw6dCgOHTo0+Ouenp6I+N3fAAAAPvh+331l3qM9rrLCd//+/dHf3x91dXVDxuvq6mLXrl1HXdPV1XXU+V1dXce8TltbW9x///1HjNfX15ezXQAAznD/8z//E7W1tSPyXmWF71hZtmzZkLvE77zzTnz4wx+OPXv2jNgH58zR29sb9fX1sXfvXl91Scj55+b8c3P+ufX09MSFF14YF1xwwYi9Z1nhO2HChKisrIzu7u4h493d3TFp0qSjrpk0aVJZ8yMiSqVSlEqlI8Zra2v9g59YTU2N80/M+efm/HNz/rlVVIzc03fLeqeqqqqYOXNmtLe3D44NDAxEe3t7NDY2HnVNY2PjkPkRES+++OIx5wMAwGgo+6sOLS0tsXDhwpg1a1bMnj07Vq9eHQcPHoxFixZFRMSCBQti6tSp0dbWFhERd911V1x33XXx8MMPx4033hjr16+Pn/70p/H444+P7CcBAIDjKDt8582bF/v27YsVK1ZEV1dXzJgxIzZv3jz4A2x79uwZckv66quvjqeffjruvffeuOeee+Iv/uIv4vnnn4/LLrvshK9ZKpWitbX1qF9/4IPP+efm/HNz/rk5/9xG4/zLfo4vAACciUbu28IAAHAaE74AAKQgfAEASEH4AgCQwmkTvmvWrIlp06ZFdXV1NDQ0xNatW487/3vf+15ceumlUV1dHZdffnls2rRpjHbKaCjn/NetWxfXXnttjB8/PsaPHx9NTU3v+88Lp7dyf///3vr162PcuHExd+7c0d0go6rc83/nnXdi8eLFMXny5CiVSnHJJZf4d8AZrNzzX716dXz0ox+Nc845J+rr62PJkiXx29/+dox2y0j58Y9/HHPmzIkpU6bEuHHj4vnnn3/fNVu2bIlPfvKTUSqV4iMf+Ug89dRT5V+4OA2sX7++qKqqKp588sniv/7rv4rbbrutOP/884vu7u6jzv/JT35SVFZWFg8++GDx6quvFvfee29x9tlnF6+88soY75yRUO7533zzzcWaNWuKHTt2FDt37iz+7u/+rqitrS3++7//e4x3zkgo9/x/78033yymTp1aXHvttcVf//Vfj81mGXHlnv+hQ4eKWbNmFTfccEPx0ksvFW+++WaxZcuWorOzc4x3zkgo9/y/853vFKVSqfjOd75TvPnmm8ULL7xQTJ48uViyZMkY75yTtWnTpmL58uXFs88+W0RE8dxzzx13/u7du4tzzz23aGlpKV599dXiG9/4RlFZWVls3ry5rOueFuE7e/bsYvHixYO/7u/vL6ZMmVK0tbUddf7nPve54sYbbxwy1tDQUPz93//9qO6T0VHu+f+xw4cPF+edd17x7W9/e7S2yCgazvkfPny4uPrqq4tvfetbxcKFC4XvGazc8//mN79ZXHTRRUVfX99YbZFRVO75L168uPj0pz89ZKylpaW45pprRnWfjK4TCd8vf/nLxSc+8YkhY/PmzSuam5vLutYp/6pDX19fbNu2LZqamgbHKioqoqmpKTo6Oo66pqOjY8j8iIjm5uZjzuf0NZzz/2PvvvtuvPfee3HBBReM1jYZJcM9/69+9asxceLEuOWWW8Zim4yS4Zz/97///WhsbIzFixdHXV1dXHbZZbFy5cro7+8fq20zQoZz/ldffXVs27Zt8OsQu3fvjk2bNsUNN9wwJnvm1Bmp9iv7T24bafv374/+/v7BP/nt9+rq6mLXrl1HXdPV1XXU+V1dXaO2T0bHcM7/j919990xZcqUI35DcPobzvm/9NJL8cQTT0RnZ+cY7JDRNJzz3717d/zHf/xHfP7zn49NmzbFG2+8EV/84hfjvffei9bW1rHYNiNkOOd/8803x/79++NTn/pUFEURhw8fjjvuuCPuueeesdgyp9Cx2q+3tzd+85vfxDnnnHNC73PK7/jCyVi1alWsX78+nnvuuaiurj7V22GUHThwIObPnx/r1q2LCRMmnOrtcAoMDAzExIkT4/HHH4+ZM2fGvHnzYvny5bF27dpTvTXGwJYtW2LlypXx2GOPxfbt2+PZZ5+NjRs3xgMPPHCqt8YZ4pTf8Z0wYUJUVlZGd3f3kPHu7u6YNGnSUddMmjSprPmcvoZz/r/30EMPxapVq+KHP/xhXHHFFaO5TUZJuef/85//PN56662YM2fO4NjAwEBERJx11lnx2muvxcUXXzy6m2bEDOf3/+TJk+Pss8+OysrKwbGPfexj0dXVFX19fVFVVTWqe2bkDOf877vvvpg/f37ceuutERFx+eWXx8GDB+P222+P5cuXR0WF+3kfVMdqv5qamhO+2xtxGtzxraqqipkzZ0Z7e/vg2MDAQLS3t0djY+NR1zQ2Ng6ZHxHx4osvHnM+p6/hnH9ExIMPPhgPPPBAbN68OWbNmjUWW2UUlHv+l156abzyyivR2dk5+PrsZz8b119/fXR2dkZ9ff1Ybp+TNJzf/9dcc0288cYbg//BExHx+uuvx+TJk0XvGWY45//uu+8eEbe//4+g3/2MFB9UI9Z+5f3c3ehYv359USqViqeeeqp49dVXi9tvv704//zzi66urqIoimL+/PnF0qVLB+f/5Cc/Kc4666zioYceKnbu3Fm0trZ6nNkZrNzzX7VqVVFVVVU888wzxa9+9avB14EDB07VR+AklHv+f8xTHc5s5Z7/nj17ivPOO6/4h3/4h+K1114rfvCDHxQTJ04svva1r52qj8BJKPf8W1tbi/POO6/4t3/7t2L37t3Fv//7vxcXX3xx8bnPfe5UfQSG6cCBA8WOHTuKHTt2FBFRPPLII8WOHTuKX/ziF0VRFMXSpUuL+fPnD87//ePM/umf/qnYuXNnsWbNmjP3cWZFURTf+MY3igsvvLCoqqoqZs+eXfznf/7n4F+77rrrioULFw6Z/93vfre45JJLiqqqquITn/hEsXHjxjHeMSOpnPP/8Ic/XETEEa/W1tax3zgjotzf//8/4XvmK/f8X3755aKhoaEolUrFRRddVHz9618vDh8+PMa7ZqSUc/7vvfde8ZWvfKW4+OKLi+rq6qK+vr744he/WPzv//7v2G+ck/KjH/3oqP8u//15L1y4sLjuuuuOWDNjxoyiqqqquOiii4p//dd/Lfu644rC/xsAAOCD75R/xxcAAMaC8AUAIAXhCwBACsIXAIAUhC8AACkIXwAAUhC+AACkIHwBAEhB+AIAkILwBQAgBeELAEAKwhcAgBT+HzgRrWHQVr4zAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NetworkX Entity graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nx.draw(conversation_with_kg.memory.kg, with_labels=True)\n",
    "# plt.show()\n",
    "\n",
    "print(conversation_with_kg.memory.kg)\n",
    "print(conversation_with_kg.memory.kg.get_triples())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
